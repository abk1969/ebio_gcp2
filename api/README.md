# API Proxy pour LLM Providers

Ce dossier contient les fonctions serverless Vercel qui servent de proxy pour contourner les restrictions CORS des providers LLM.

## ğŸ¯ Pourquoi un proxy ?

La plupart des APIs LLM (Anthropic, OpenAI, Mistral, etc.) bloquent les appels directs depuis le navigateur pour des raisons de sÃ©curitÃ© (CORS - Cross-Origin Resource Sharing). 

Le proxy backend permet de :
- âœ… Faire les appels API depuis le serveur (pas de CORS)
- âœ… ProtÃ©ger les clÃ©s API (bien que dans notre cas elles soient encore cÃ´tÃ© client)
- âœ… GÃ©rer les erreurs de maniÃ¨re centralisÃ©e
- âœ… Ajouter des logs et du monitoring

## ğŸ“ Structure

```
api/
â”œâ”€â”€ llm-proxy.ts       # Proxy principal pour tous les providers
â””â”€â”€ README.md          # Cette documentation
```

## ğŸ”§ Fonctionnement

### Endpoint

```
POST /api/llm-proxy?provider={provider}
```

### Providers supportÃ©s

- `anthropic` - Claude (Anthropic)
- `openai` - GPT-4o et modÃ¨les OpenAI
- `mistral` - Mistral Large
- `deepseek` - DeepSeek-V3
- `qwen` - Qwen Max (Alibaba)
- `xai` - Grok (xAI)
- `groq` - Llama via Groq

### Headers requis

```
Content-Type: application/json
x-api-key: YOUR_API_KEY
```

### Exemple d'utilisation

```typescript
const response = await fetch('/api/llm-proxy?provider=anthropic', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'x-api-key': 'sk-ant-...',
  },
  body: JSON.stringify({
    model: 'claude-sonnet-4-20250514',
    messages: [{ role: 'user', content: 'Hello!' }],
    max_tokens: 1000,
  }),
});
```

## ğŸš€ DÃ©ploiement

Le proxy est automatiquement dÃ©ployÃ© avec l'application sur Vercel. Aucune configuration supplÃ©mentaire n'est nÃ©cessaire.

### Configuration Vercel

Le fichier `vercel.json` contient la configuration pour router les requÃªtes `/api/*` vers les fonctions serverless :

```json
{
  "rewrites": [
    {
      "source": "/api/:path*",
      "destination": "/api/:path*"
    }
  ]
}
```

## ğŸ”’ SÃ©curitÃ©

âš ï¸ **Note importante** : Actuellement, les clÃ©s API sont stockÃ©es cÃ´tÃ© client (localStorage avec chiffrement XOR). 

Pour une sÃ©curitÃ© optimale en production, il faudrait :
1. Stocker les clÃ©s API cÃ´tÃ© serveur (variables d'environnement Vercel)
2. ImplÃ©menter une authentification utilisateur
3. Associer chaque utilisateur Ã  ses propres clÃ©s API
4. Ne jamais exposer les clÃ©s API au client

## ğŸ§ª Tests

Pour tester le proxy localement :

```bash
# Installer Vercel CLI
npm i -g vercel

# Lancer le serveur de dÃ©veloppement Vercel
vercel dev
```

Puis tester avec curl :

```bash
curl -X POST http://localhost:3000/api/llm-proxy?provider=anthropic \
  -H "Content-Type: application/json" \
  -H "x-api-key: YOUR_API_KEY" \
  -d '{"model":"claude-sonnet-4-20250514","messages":[{"role":"user","content":"Hello"}],"max_tokens":100}'
```

## ğŸ“Š Monitoring

Les logs du proxy sont visibles dans :
- **DÃ©veloppement** : Console du terminal
- **Production** : Vercel Dashboard > Functions > Logs

## ğŸ› Debugging

Si le proxy ne fonctionne pas :

1. VÃ©rifier que le provider est supportÃ©
2. VÃ©rifier que l'API key est valide
3. Consulter les logs Vercel
4. VÃ©rifier la configuration CORS dans `llm-proxy.ts`

## ğŸ“ Providers qui ne nÃ©cessitent PAS de proxy

- âœ… **Google Gemini** - CORS configurÃ© pour les appels directs
- âœ… **Ollama** - Serveur local
- âœ… **LM Studio** - Serveur local

Ces providers peuvent Ãªtre appelÃ©s directement depuis le navigateur.

