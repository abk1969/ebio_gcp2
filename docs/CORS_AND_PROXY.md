# ğŸŒ CORS et Proxy Backend - Guide Utilisateur

## ğŸ¤” Qu'est-ce que CORS ?

**CORS** (Cross-Origin Resource Sharing) est une sÃ©curitÃ© du navigateur qui empÃªche un site web d'appeler des APIs externes directement depuis le navigateur.

### Pourquoi c'est important ?

La plupart des providers LLM (Anthropic, OpenAI, Mistral, etc.) bloquent les appels directs depuis le navigateur pour des raisons de sÃ©curitÃ©. C'est **normal et attendu**.

## âœ… Solutions implÃ©mentÃ©es

### 1. Proxy Backend Vercel (Production)

En production sur Vercel, l'application utilise automatiquement un **proxy backend** qui :
- âœ… Contourne les restrictions CORS
- âœ… Fonctionne automatiquement sans configuration
- âœ… Supporte tous les providers

**Providers utilisant le proxy en production :**
- ğŸ¤– Anthropic (Claude)
- ğŸ¤– OpenAI (GPT-4o)
- ğŸ¤– Mistral AI
- ğŸ¤– DeepSeek
- ğŸ¤– Qwen (Alibaba)
- ğŸ¤– xAI (Grok)
- ğŸ¤– Groq

### 2. Providers fonctionnant sans proxy

Certains providers ont des configurations CORS permissives et fonctionnent directement :
- âœ… **Google Gemini** - Fonctionne partout
- âœ… **Ollama** - Serveur local (http://localhost:11434)
- âœ… **LM Studio** - Serveur local (http://localhost:1234)

## ğŸ  DÃ©veloppement Local

### Pour Anthropic (Claude)

En dÃ©veloppement local, Anthropic nÃ©cessite un proxy local. Deux options :

#### Option 1 : Script automatique (Windows)
```bash
# Double-cliquer sur le fichier
start-proxy.bat
```

#### Option 2 : Commande npm
```bash
# Dans un terminal sÃ©parÃ©
npm run dev:proxy
```

### Pour les autres providers

En dÃ©veloppement local, les autres providers (OpenAI, Mistral, etc.) peuvent rencontrer des erreurs CORS. C'est **normal**.

**Solutions :**
1. âœ… **Utiliser Gemini** - Fonctionne sans proxy
2. âœ… **Utiliser Ollama/LM Studio** - Serveurs locaux
3. âš ï¸ **Tester en production** - Le proxy Vercel fonctionne automatiquement

## ğŸ¯ Recommandations par environnement

### ğŸŒ Production (Vercel)
- âœ… **Tous les providers fonctionnent** grÃ¢ce au proxy backend
- âœ… Aucune configuration nÃ©cessaire
- âœ… ExpÃ©rience utilisateur optimale

### ğŸ’» DÃ©veloppement Local
- âœ… **Gemini** - RecommandÃ© pour le dÃ©veloppement
- âœ… **Ollama/LM Studio** - Pour les tests locaux
- âš ï¸ **Anthropic** - NÃ©cessite le proxy local
- âš ï¸ **Autres providers** - Peuvent avoir des problÃ¨mes CORS

## ğŸ”§ Configuration dans l'interface

L'application affiche automatiquement un message d'information pour les providers nÃ©cessitant un proxy :

```
â„¹ï¸ Information importante

Ce provider utilise un proxy backend pour contourner les restrictions CORS.

âœ… En production (Vercel) : Fonctionne automatiquement via le proxy Vercel
âš ï¸ En dÃ©veloppement local : Lancez le proxy avec npm run dev:proxy
```

## ğŸ› RÃ©solution des problÃ¨mes

### Erreur : "Failed to fetch" ou "CORS policy"

**En production :**
- âœ… Le proxy devrait fonctionner automatiquement
- ğŸ” VÃ©rifier que l'API key est valide
- ğŸ” Consulter les logs Vercel

**En dÃ©veloppement local :**
- âœ… Utiliser Gemini Ã  la place
- âœ… Lancer le proxy local pour Anthropic
- âœ… Tester en production pour les autres providers

### Erreur : "Proxy timeout" (Anthropic en local)

Le proxy local n'est pas lancÃ©. Solutions :
1. Lancer `npm run dev:proxy` dans un terminal sÃ©parÃ©
2. Double-cliquer sur `start-proxy.bat` (Windows)
3. Utiliser Gemini Ã  la place

### Erreur : "API key is required"

L'API key n'est pas configurÃ©e ou invalide :
1. Aller dans **Settings**
2. SÃ©lectionner le provider
3. Entrer une clÃ© API valide
4. Cliquer sur **Sauvegarder**

## ğŸ“Š Tableau rÃ©capitulatif

| Provider | Production | Dev Local | Proxy Requis |
|----------|-----------|-----------|--------------|
| **Gemini** | âœ… Direct | âœ… Direct | âŒ Non |
| **Anthropic** | âœ… Proxy Vercel | âš ï¸ Proxy Local | âœ… Oui |
| **OpenAI** | âœ… Proxy Vercel | âš ï¸ CORS | âœ… Oui |
| **Mistral** | âœ… Proxy Vercel | âš ï¸ CORS | âœ… Oui |
| **DeepSeek** | âœ… Proxy Vercel | âš ï¸ CORS | âœ… Oui |
| **Qwen** | âœ… Proxy Vercel | âš ï¸ CORS | âœ… Oui |
| **xAI** | âœ… Proxy Vercel | âš ï¸ CORS | âœ… Oui |
| **Groq** | âœ… Proxy Vercel | âš ï¸ CORS | âœ… Oui |
| **Ollama** | âœ… Direct | âœ… Direct | âŒ Non |
| **LM Studio** | âœ… Direct | âœ… Direct | âŒ Non |

## ğŸš€ Pour aller plus loin

### AmÃ©lioration de la sÃ©curitÃ©

Pour une sÃ©curitÃ© optimale en production, il faudrait :
1. Stocker les clÃ©s API cÃ´tÃ© serveur (variables d'environnement)
2. ImplÃ©menter une authentification utilisateur
3. Associer chaque utilisateur Ã  ses propres clÃ©s API
4. Ne jamais exposer les clÃ©s API au client

### Architecture actuelle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Navigateur â”‚
â”‚   (Client)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Appel API
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Proxy     â”‚
â”‚  Vercel     â”‚ â† Contourne CORS
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Appel API
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Provider   â”‚
â”‚  LLM API    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture recommandÃ©e (future)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Navigateur â”‚
â”‚   (Client)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Auth Token
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend   â”‚
â”‚   + Auth    â”‚ â† ClÃ©s API stockÃ©es ici
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Appel API avec clÃ© serveur
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Provider   â”‚
â”‚  LLM API    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š Ressources

- [Documentation CORS (MDN)](https://developer.mozilla.org/fr/docs/Web/HTTP/CORS)
- [Vercel Serverless Functions](https://vercel.com/docs/functions/serverless-functions)
- [Anthropic API Documentation](https://docs.anthropic.com/)
- [OpenAI API Documentation](https://platform.openai.com/docs/)

