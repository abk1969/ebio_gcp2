# Corrections des probl√®mes LLM - EBIOS RM

## üéØ Objectif
Corriger les probl√®mes qui emp√™chaient les LLM autres que Gemini de fonctionner correctement, tout en pr√©servant le fonctionnement de Gemini.

## üîç Probl√®mes identifi√©s

### 1. **Parsing JSON d√©faillant**
- **Probl√®me** : La fonction `parseJsonFromText` √©tait trop basique et √©chouait souvent
- **Solution** : Am√©lioration de l'algorithme de parsing avec plusieurs strat√©gies de fallback

### 2. **Instructions syst√®me insuffisantes**
- **Probl√®me** : Les instructions pour forcer le format JSON n'√©taient pas assez strictes
- **Solution** : Instructions renforc√©es et sp√©cifiques pour chaque type de LLM

### 3. **Gestion d'erreurs incompl√®te**
- **Probl√®me** : Erreurs g√©n√©riques sans d√©tails pour diagnostiquer les probl√®mes
- **Solution** : Logs d√©taill√©s et messages d'erreur sp√©cifiques par fournisseur

### 4. **API Anthropic mal configur√©e**
- **Probl√®me** : Anthropic utilise un format diff√©rent (param√®tre `system` s√©par√©)
- **Solution** : Adaptation du format de requ√™te pour Anthropic

### 5. **Mod√®les obsol√®tes**
- **Probl√®me** : Certains mod√®les par d√©faut n'existaient plus
- **Solution** : Mise √† jour des mod√®les par d√©faut

## üõ†Ô∏è Corrections apport√©es

### 1. **Am√©lioration du parsing JSON** (`services/llmService.ts`)
```typescript
// Nouvelle fonction parseJsonFromText avec :
- Logs d√©taill√©s pour le debugging
- Nettoyage automatique des candidats
- Multiples strat√©gies d'extraction
- Messages d'erreur informatifs
```

### 2. **Instructions syst√®me renforc√©es**
```typescript
// buildJsonUserPrompt et buildJsonSystemInstruction am√©lior√©es
- Instructions plus strictes et d√©taill√©es
- R√®gles sp√©cifiques pour chaque LLM
- Gestion des sch√©mas JSON
```

### 3. **Correction sp√©cifique Anthropic**
```typescript
// Format de requ√™te corrig√© pour Claude
- Param√®tre `system` s√©par√© des messages
- Gestion correcte de l'API Anthropic
```

### 4. **Gestion d'erreurs am√©lior√©e**
```typescript
// Dans agentOrchestrator.ts et llmService.ts
- Logs d√©taill√©s par fournisseur
- Messages d'erreur sp√©cifiques
- Validation des configurations
- Conseils de d√©pannage
```

### 5. **Services locaux optimis√©s**
```typescript
// Ollama et LM Studio
- Messages d'erreur sp√©cifiques pour les services locaux
- V√©rification de connectivit√© am√©lior√©e
- Conseils de configuration
```

## üß™ Outils de test ajout√©s

### 1. **LLMTester** (`utils/llmTester.ts`)
- Classe utilitaire pour tester tous les fournisseurs
- Tests automatis√©s avec rapports d√©taill√©s
- Validation de la g√©n√©ration JSON

### 2. **Composant de test UI** (`components/LLMTester.tsx`)
- Interface graphique pour tester les LLM
- Tests individuels ou en lot
- Affichage des r√©sultats en temps r√©el

### 3. **Script de test CLI** (`scripts/test-llm.js`)
- Test en ligne de commande
- Int√©gration possible dans CI/CD
- Rapports format√©s

## üìã Utilisation

### Test depuis l'interface
1. Aller dans **Param√®tres**
2. Cliquer sur **üß™ Tester tous les LLM**
3. Voir les r√©sultats en temps r√©el

### Test en ligne de commande
```bash
# Tester un fournisseur sp√©cifique
node scripts/test-llm.js gemini

# Tester tous les fournisseurs configur√©s
node scripts/test-llm.js
```

## ‚úÖ Validation

### Tests effectu√©s
- [x] Gemini : Fonctionnement pr√©serv√©
- [x] OpenAI : Correction du parsing JSON
- [x] Anthropic : Correction du format API
- [x] Mistral : Am√©lioration gestion d'erreurs
- [x] DeepSeek : Logs ajout√©s
- [x] Autres fournisseurs : Optimisations g√©n√©rales

### R√©gressions √©vit√©es
- ‚úÖ Gemini continue de fonctionner normalement
- ‚úÖ Pas de changement dans l'interface utilisateur existante
- ‚úÖ Compatibilit√© ascendante maintenue

## üîß Configuration recommand√©e

### Fournisseurs cloud
- V√©rifier que les cl√©s API sont valides
- Utiliser les mod√®les recommand√©s
- Surveiller les quotas

### Fournisseurs locaux
- S'assurer qu'Ollama/LM Studio sont d√©marr√©s
- V√©rifier les URLs de connexion
- Tester la connectivit√© r√©seau

## üìä Monitoring

### Logs √† surveiller
```
[LLM] Initialisation du service pour le fournisseur: {provider}
[Agent] Appel de l'agent avec le fournisseur: {provider}
[{provider}] Tentative de parsing JSON
[{provider}] JSON pars√© avec succ√®s
```

### Erreurs communes
- `Cl√© API manquante` : Configurer la cl√© dans les param√®tres
- `Impossible de se connecter` : V√©rifier la connectivit√©
- `JSON attendu` : Probl√®me de parsing, voir les logs d√©taill√©s

## üöÄ Prochaines √©tapes

1. **Monitoring en production** : Surveiller les logs pour d√©tecter les probl√®mes
2. **Tests r√©guliers** : Utiliser le testeur LLM p√©riodiquement
3. **Optimisations** : Am√©liorer les performances selon les retours
4. **Nouveaux fournisseurs** : Ajouter d'autres LLM si n√©cessaire

## üìù Notes techniques

- Toutes les modifications sont r√©trocompatibles
- Les logs peuvent √™tre d√©sactiv√©s en production si n√©cessaire
- Le cache des services LLM est maintenu pour les performances
- La validation des configurations est renforc√©e
